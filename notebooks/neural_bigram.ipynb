{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0835929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F    \n",
    "from torch.optim import AdamW\n",
    "from torch.optim import ASGD\n",
    "from torch.amp import GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(\"..\", \"checkpoints\", \"neural_bigram\")\n",
    "LOG_DIR = os.path.join(\"..\", \"logs\", \"neural_bigram\")\n",
    "RESULTS_DIR = os.path.join(\"..\", \"results\", \"neural_bigram\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295d588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to the path to allow imports\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from neural_bigram import NeuralBigram, ConfigNeuralBigram\n",
    "from train_mj import train, evaluate, evaluate_ppl, ConfigTrain\n",
    "from utils import init_dataloader, WarmupThenCosine, set_seed, count_params, save_checkpoint, load_checkpoint\n",
    "from bpe_hf import train_bytelevel_bpe, train_and_encode_tokenizer, load_tokenizer, SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db0df6",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f18254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train text charachters length:  883780\n",
      "train text words count:  158605\n",
      "train text first 100 chars:  The Tragedy of Antony and Cleopatra\n",
      "\n",
      "\n",
      "Dramatis Personae\n",
      "\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "OCTAVIUS CAESAR\n",
      "M. AEMILIUS L\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/\"\n",
    "# without nl\n",
    "# train_file = \"Shakespeare_clean_train.txt\"\n",
    "# val_file = \"Shakespeare_clean_valid.txt\"\n",
    "# test_file = \"Shakespeare_clean_test.txt\"\n",
    "\n",
    "# with nl\n",
    "train_file = \"Shakespeare_clean_w_nl_train.txt\"\n",
    "val_file = \"Shakespeare_clean_w_nl_valid.txt\"\n",
    "test_file = \"Shakespeare_clean_w_nl_test.txt\"\n",
    "\n",
    "train_file = os.path.join(data_dir, train_file)\n",
    "test_file = os.path.join(data_dir, test_file)\n",
    "val_file = os.path.join(data_dir, val_file)\n",
    "\n",
    "with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_text = f.read().strip()\n",
    "\n",
    "with open(val_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    val_text = f.read().strip()\n",
    "\n",
    "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_text = f.read().strip()\n",
    "\n",
    "print(\"train text charachters length: \", len(train_text))\n",
    "print(\"train text words count: \", len(train_text.split()))\n",
    "print(\"train text first 100 chars: \", train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176563d",
   "metadata": {},
   "source": [
    "## initialize tokensizer and data embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1027401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  460\n",
      "BPE tokenizer vocab:  [('°', 112), ('õ', 181), ('ł', 258), ('N', 49), ('ĥ', 229), ('â', 162), ('Ķ', 246), ('Þ', 158), ('ĠR', 391), ('<', 31), ('Æ', 134), ('Ĳ', 242), (\"'s\", 340), ('ú', 186), ('Ê', 138), ('Ġk', 381), ('Ġse', 401), ('3', 22), ('ge', 443), ('y', 92), ('Ã', 131), ('%', 8), ('¨', 105), ('T', 55), ('al', 366), ('ome', 363), ('C', 38), ('<bos>', 2), ('im', 338), ('Ġkn', 445), ('?', 34), ('d', 71), (':', 29), ('Ġthou', 369), ('id', 365), ('Ġyour', 373), ('/', 18), ('ad', 351), ('4', 23), ('es', 284), ('ing', 310), ('å', 165), ('ç', 167), ('¥', 102), ('st', 309), ('}', 96), ('1', 20), ('ĠO', 329), ('o', 82), ('c', 70), ('ö', 182), ('Ġne', 441), ('R', 53), ('Ö', 150), ('Đ', 208), ('Ġthis', 376), ('Ġon', 379), ('Ġwill', 400), ('Ģ', 226), ('ith', 336), ('ly', 394), ('.', 17), ('ĠT', 291), ('Ħ', 230), ('IA', 433), ('a', 68), ('ght', 354), ('Ġy', 292), ('ĠE', 345), ('AR', 396), ('k', 78), ('p', 83), ('D', 39), ('ĳ', 243), ('ha', 271), ('ď', 207), ('İ', 240), ('e', 72), ('Ô', 148), ('à', 160), ('Ā', 192), ('½', 125), ('ĠF', 370), ('Ġhim', 377), (',', 15), ('L', 47), ('<pad>', 0), ('Ġno', 420), ('ó', 179), ('È', 136), ('her', 380), ('Ë', 139), ('Č', 204), ('Ì', 140), ('US', 348), ('×', 151), ('Ċ', 202), ('er', 276), ('æ', 166), ('ă', 195)]\n",
      "train ids length:  498262\n",
      "train ids first 100 ids:  [364, 291, 378, 74, 332, 92, 303, 293, 359, 288, 92, 299, 313, 322, 82, 83, 311, 378, 202, 202, 202, 39, 378, 80, 311, 274, 372, 276, 86, 288, 68, 72, 202, 202, 202, 202, 48, 396, 46, 293, 49, 55, 419, 60, 202, 50, 38, 55, 36, 57, 399, 426, 40, 54, 396, 202, 48, 17, 293, 40, 48, 44, 47, 399, 355, 40, 51, 44, 39, 348, 202, 87, 361, 88, 80, 89, 334, 86, 17, 202, 202, 202, 54, 40, 59, 55, 348, 372, 50, 48, 51, 40, 399, 202, 202, 202, 39, 50, 48, 44]\n",
      "train ids first 100 individual tokens:  ['ĠThe', 'ĠT', 'ra', 'g', 'ed', 'y', 'Ġof', 'ĠA', 'nt', 'on', 'y', 'Ġand', 'ĠC', 'le', 'o', 'p', 'at', 'ra', 'Ċ', 'Ċ', 'Ċ', 'D', 'ra', 'm', 'at', 'is', 'ĠP', 'er', 's', 'on', 'a', 'e', 'Ċ', 'Ċ', 'Ċ', 'Ċ', 'M', 'AR', 'K', 'ĠA', 'N', 'T', 'ON', 'Y', 'Ċ', 'O', 'C', 'T', 'A', 'V', 'IUS', 'ĠCA', 'E', 'S', 'AR', 'Ċ', 'M', '.', 'ĠA', 'E', 'M', 'I', 'L', 'IUS', 'ĠL', 'E', 'P', 'I', 'D', 'US', 'Ċ', 't', 'ri', 'u', 'm', 'v', 'ir', 's', '.', 'Ċ', 'Ċ', 'Ċ', 'S', 'E', 'X', 'T', 'US', 'ĠP', 'O', 'M', 'P', 'E', 'IUS', 'Ċ', 'Ċ', 'Ċ', 'D', 'O', 'M', 'I']\n"
     ]
    }
   ],
   "source": [
    "N_MERGES = 200\n",
    "MIN_FREQ = 2\n",
    "SPECIAL_TOKENS = SPECIAL_TOKENS.copy()\n",
    "PAD_TOKEN = SPECIAL_TOKENS.get(\"pad\", \"<pad>\")\n",
    "BOS_TOKEN = SPECIAL_TOKENS.get(\"bos\", \"<bos>\")\n",
    "EOS_TOKEN = SPECIAL_TOKENS.get(\"eos\", \"<eos>\")\n",
    "\n",
    "\n",
    "bpe_tokenizer = train_bytelevel_bpe(\n",
    "    merges=N_MERGES,\n",
    "    min_frequency=MIN_FREQ,\n",
    "    files=[train_file],\n",
    "    lowercase=False,\n",
    "    add_prefix_space=True,\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    save_filename=f\"shakespeare_bpe_{N_MERGES}\"\n",
    ")\n",
    "\n",
    "encode = partial(bpe_tokenizer.encode, add_special_tokens=False)\n",
    "decode = bpe_tokenizer.decode\n",
    "vocab_size = bpe_tokenizer.get_vocab_size()\n",
    "print(\"vocab size: \", vocab_size)\n",
    "print(\"BPE tokenizer vocab: \", list(bpe_tokenizer.get_vocab().items())[:100])\n",
    "\n",
    "PAD_TOKEN_ID = bpe_tokenizer.token_to_id(PAD_TOKEN) \n",
    "BOS_TOKEN_ID = bpe_tokenizer.token_to_id(BOS_TOKEN)\n",
    "EOS_TOKEN_ID = bpe_tokenizer.token_to_id(EOS_TOKEN)\n",
    "\n",
    "train_ids = encode(train_text).ids\n",
    "val_ids = encode(val_text).ids\n",
    "test_ids = encode(test_text).ids\n",
    "\n",
    "print(\"train ids length: \", len(train_ids))\n",
    "print(\"train ids first 100 ids: \", train_ids[:100])\n",
    "print(\"train ids first 100 individual tokens: \", [bpe_tokenizer.id_to_token(i) for i in train_ids[:100]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed89b0",
   "metadata": {},
   "source": [
    "## define the model's and training parameters (configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ed5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  460\n",
      "number of model parameters:  211600\n",
      "number of batches in train loader:  15570\n",
      "number of batches in test loader:  1860\n",
      "number of batches in val loader:  1849\n",
      "log dir:  ../logs/neural_bigram/nbigram_bpe200/0.003_drop0.1_250824_215037\n",
      "checkpoint dir:  ../checkpoints/neural_bigram\n",
      "checkpoint best filename:  nbigram_bpe200_best.pt\n",
      "checkpoint last filename:  nbigram_bpe200_last.pt\n"
     ]
    }
   ],
   "source": [
    "SEED = 10\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# hyperparameters \n",
    "# tokenizer\n",
    "# N_MERGES = 200\n",
    "# MIN_FREQ = 2\n",
    "# data\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 128\n",
    "\n",
    "#  model\n",
    "DROPOUT = 0.1\n",
    "# optimizer\n",
    "LR = 3e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "# lr scheduler\n",
    "ETA_MIN = 1e-7\n",
    "\n",
    "# training loop\n",
    "EPOCHS = 60\n",
    "ES_PATIENCE = 5\n",
    "ES_TOLERANCE = 1e-6\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "USE_AMP = False # IMPORTANT: set to False if training on CPU training \n",
    "\n",
    "# checkpoints\n",
    "EVAL_INTERVAL = 2   # evaluate every n epochs\n",
    "CKPT_INTERVAL = 10  # save checkpoint every n epochs\n",
    "\n",
    "# make unique checkpoint file prefix from the parameters\n",
    "ckpt_file_prefix = f\"nbigram_bpe{N_MERGES}\"\n",
    "all_hparams_cfg_filename = f\"{ckpt_file_prefix}_hparams.json\"\n",
    "\n",
    "# config for training\n",
    "cfg_train = ConfigTrain(device=DEVICE,\n",
    "                    epochs=EPOCHS, \n",
    "                    early_stop_patience=ES_PATIENCE,\n",
    "                    early_stop_tolerance=ES_TOLERANCE,\n",
    "                    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "                    # max_grad_norm=data.get(\"max_grad_norm\", 1.0),\n",
    "                    use_amp=USE_AMP,\n",
    "                    seed=SEED,\n",
    "                    eval_interval=EVAL_INTERVAL,\n",
    "                    ckpt_interval=CKPT_INTERVAL,\n",
    "                    ckpt_dir=CHECKPOINT_DIR,\n",
    "                    ckpt_best_filename=f\"{ckpt_file_prefix}_best.pt\",\n",
    "                    ckpt_last_filename=f\"{ckpt_file_prefix}_last.pt\",\n",
    "                    log_dir=os.path.join(LOG_DIR, f\"{ckpt_file_prefix}\", f'{LR}_drop{DROPOUT}_{time.strftime(\"%y%m%d_%H%M%S\")}'),\n",
    "                    )\n",
    "# config for model\n",
    "cfg_model = ConfigNeuralBigram(\n",
    "                    vocab_size=vocab_size,\n",
    "                    dropout=DROPOUT\n",
    "                    )\n",
    "\n",
    "# model \n",
    "print(\"vocab size: \", vocab_size)\n",
    "print(\"number of model parameters: \", count_params(NeuralBigram(config=cfg_model)))\n",
    "\n",
    "# print all log and checkpoint dir and \n",
    "print(\"number of batches in train loader: \", len(train_ids)//BATCH_SIZE)\n",
    "print(\"number of batches in test loader: \", len(test_ids)//BATCH_SIZE)\n",
    "print(\"number of batches in val loader: \", len(val_ids)//BATCH_SIZE)\n",
    "\n",
    "print(\"log dir: \", cfg_train.log_dir)\n",
    "print(\"checkpoint dir: \", cfg_train.ckpt_dir)\n",
    "print(\"checkpoint best filename: \", cfg_train.ckpt_best_filename)\n",
    "print(\"checkpoint last filename: \", cfg_train.ckpt_last_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a09a36",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(cfg_train.seed)\n",
    "train_loader = init_dataloader(train_ids, BLOCK_SIZE, BATCH_SIZE, train=True, shuffle=True)\n",
    "test_loader = init_dataloader(test_ids, BLOCK_SIZE, BATCH_SIZE, train=False, shuffle=True)\n",
    "val_loader = init_dataloader(val_ids, BLOCK_SIZE, BATCH_SIZE, train=False, shuffle=True)\n",
    "\n",
    "\n",
    "model = NeuralBigram(cfg_model)\n",
    "model.to(cfg_train.device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "lr_scheduler = WarmupThenCosine(optimizer, warmup_steps=500, T_max=cfg_train.epochs * len(train_loader) // max(1, cfg_train.grad_accum_steps), eta_min=ETA_MIN)\n",
    "# rl_scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs * len(train_loader) // max(1, cfg.grad_accum_steps), eta_min=ETA_MIN)\n",
    "# rl_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=2 * len(train_loader) // max(1, cfg.grad_accum_steps), T_mult=2, eta_min=ETA_MIN)\n",
    "\n",
    "scaler = GradScaler(enabled=cfg_train.use_amp)\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=cfg_train.log_dir, flush_secs=5)\n",
    "\n",
    "# add model graph to tensorboard\n",
    "# write model graph\n",
    "dummy_input = torch.zeros((1, BLOCK_SIZE), dtype=torch.long, device=cfg_train.device)\n",
    "summary_writer.add_graph(model.eval(), dummy_input)  # eval() avoids dropout noise\n",
    "summary_writer.flush()\n",
    "summary_writer.close()\n",
    "\n",
    "# compile the model (can skip if not needed)\n",
    "model.compile(mode=\"reduce-overhead\")\n",
    "\n",
    "# inspect the model\n",
    "print(f\"Model parameters: {count_params(model):_}\")\n",
    "print(\"vocab size: \", vocab_size)\n",
    "print(f\"excepted iniital CE loss( uniform model ): {-np.log(1/vocab_size):.4f}, ppl: {np.exp(-np.log(1/vocab_size)):.4f}\")\n",
    "# evaluate the untrained model\n",
    "val_loss = evaluate(model, val_loader, device=cfg_train.device)\n",
    "val_ppl = np.exp(val_loss)\n",
    "print(f\"not trained model Initial validation loss: {val_loss:.4f}, ppl: {val_ppl:.4f}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c455c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c438e91fbf54af49176dbefffc03570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87e15bb8e09416789271e5d8c897c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c2eeec959e4fcf86afa73627c6ca8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f73943f28c343cba0f01471266fb1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d10a5d11c2349c9b865fc1b62c5e97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ee86bd05bc4b66993d745edf417aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f3b72908dc48f5b15e28541d6cc33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0f6597aed04bcd96005f81a12297ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6975db2162dc45afa011a46978179842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29e25c54a4b40e4bbdb41e10da313df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05eaefed15549cfb5df418675c1d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24553e8b347c4f508d16afa98ba99c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4161a1460854167bece6d325591e734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1a59b72cd24aa1aa5e6b15a21ee4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c1f20a5aac445a8be27194576025e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14ff4973c9a42b18264508628ee6ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfaae6bbc6e45b68cc64481750652ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea079c9e7204df58bfa27a0557872d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e8d577fc6c48dba8a8361ed6152e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5d5357ac6c4671a8ba288511b054fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/60:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = train(model, train_loader, val_loader, cfg=cfg_train,\n",
    "                      optimizer=optimizer, scheduler=lr_scheduler, scaler=scaler,\n",
    "                      writer=summary_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6866d",
   "metadata": {},
   "source": [
    "### save all parameters used in the model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b195b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the hyperparameters into a json file\n",
    "rl_scheduler_state_dict = lr_scheduler.state_dict() if lr_scheduler.__class__.__name__ != \"WarmupThenCosine\" else {\n",
    "            k:v for k, v in lr_scheduler.state_dict().items() if k != \"cosine\"}\n",
    "all_hparams_cfg = {\n",
    "    \"tokenizer\" : {\n",
    "        \"type\": \"bytelevel_bpe\",\n",
    "        \"n_merges\": N_MERGES,\n",
    "        \"min_frequency\": MIN_FREQ,\n",
    "        \"special_tokens\": SPECIAL_TOKENS,\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"block_size\": BLOCK_SIZE,\n",
    "    },\n",
    "    \"model\": vars(cfg_model).copy(),\n",
    "    \"optimizer\": {\n",
    "        \"type\": optimizer.__class__.__name__,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"defaults\": optimizer.defaults,\n",
    "    },\n",
    "    \"lr_scheduler\": {\n",
    "        \"type\": lr_scheduler.__class__.__name__,\n",
    "        \"eta_min\": ETA_MIN,\n",
    "        \"state_dict\": rl_scheduler_state_dict,\n",
    "    },\n",
    "    \"scaler\": {\n",
    "        \"type\": scaler.__class__.__name__,\n",
    "        \"enabled\": scaler.is_enabled(),\n",
    "        \"state_dict\": scaler.state_dict(),\n",
    "    },\n",
    "    \"training\": vars(cfg_train).copy(),\n",
    "    \"results\": train_results,\n",
    "}\n",
    "\n",
    "# save it to checkpoint dir, same prefix as checkpoint files\n",
    "with open(os.path.join(CHECKPOINT_DIR, all_hparams_cfg_filename), \"w\") as f:\n",
    "    json.dump(all_hparams_cfg, f, indent=4)\n",
    "\n",
    "# write it to tensorboard\n",
    "summary_writer.add_text(\"config/json\", \"```json\\n\" + json.dumps(all_hparams_cfg, indent=2, sort_keys=True) + \"\\n```\", global_step=0)\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1da283",
   "metadata": {},
   "source": [
    "## results and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee7297",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# make train_loss, train_ppl, val_ppl, val_loss and epochs into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_results\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m history_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(history)\n\u001b[1;32m      5\u001b[0m display(history_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_results' is not defined"
     ]
    }
   ],
   "source": [
    "# make train_loss, train_ppl, val_ppl, val_loss and epochs into a DataFrame\n",
    "history = train_results[\"history\"]\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "display(history_df.head())\n",
    "display(history_df.tail())\n",
    "# plot results \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['epochs'], history_df[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history_df['epochs'], history_df[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(history_df['epochs'][::int(len(history_df['epochs'])//9)+1])  \n",
    "plt.title(\"Training Loss and Validation Perplexity\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# plot perplexity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['epochs'], history_df[\"train_ppl\"], label=\"Train PPL\")\n",
    "plt.plot(history_df['epochs'], history_df[\"val_ppl\"], label=\"Val PPL\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.xticks(history_df['epochs'][::int(len(history_df['epochs'])//9)+1])  \n",
    "plt.title(\"Validation Perplexity Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa218a29",
   "metadata": {},
   "source": [
    "### generate text from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7ec1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>loss(NLL)</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>3.629925</td>\n",
       "      <td>37.709980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>3.404252</td>\n",
       "      <td>30.091774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>3.465237</td>\n",
       "      <td>31.984036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  loss(NLL)  perplexity\n",
       "0  train   3.629925   37.709980\n",
       "1    val   3.404252   30.091774\n",
       "2   test   3.465237   31.984036"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare with exponentiated average negative log likelihood\n",
    "test_nll = evaluate(model, test_loader, cfg_train.device)\n",
    "test_ppl = np.exp(test_nll)\n",
    "\n",
    "val_ppl = train_results[\"best_val_ppl\"]\n",
    "train_ppl = train_results[\"best_train_ppl\"]\n",
    "\n",
    "val_nll = torch.log(torch.tensor(val_ppl)).item()\n",
    "train_nll = torch.log(torch.tensor(train_ppl)).item()\n",
    "\n",
    "# display in a DataFrame\n",
    "test_results_df = pd.DataFrame({\n",
    "    \"split\": [\"train\", \"val\", \"test\"],\n",
    "    \"loss(NLL)\": [train_nll, val_nll, test_nll],\n",
    "    \"perplexity\": [train_ppl, val_ppl, test_ppl],\n",
    "})\n",
    "display(test_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d51c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> <context>\n",
      " -------------------- \n",
      "<generated_text>\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      ">>>> vil\n",
      "Free me so far\n",
      "--------------------\n",
      ":\n",
      "\n",
      "CLive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SCBRATIUS\n",
      "BAllyems\n",
      "TAVence,\n",
      "TUS\n",
      "He.\n",
      "Hation,\n",
      "HERe: apranian.\n",
      "\n",
      "TZO\n",
      "ORNTONAnders, and mortist is my say,\n",
      "\n",
      "\n",
      "\n",
      "TORIt.\n",
      "Fa, b\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      ">>>> it on nature's misch\n",
      "--------------------\n",
      "\n",
      "Forteepratters offessiance,\n",
      "Beman: for agian. The soul,\n",
      "What should not dapres.\n",
      "Whens of us,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DIUS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PAnsien\n",
      "These and lad.\n",
      "\n",
      "Caulage in thy make the per\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      ">>>> s of success.\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "There,\n",
      "And,\n",
      "TRDove to-s and tont, to dam; five.\n",
      "Whathoubalk.\n",
      "Younicep,--y, and gost be feet hathines.\n",
      "\n",
      "How not.\n",
      "DEODELEOfore mocksentss. Friark you, jeres\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      ">>>>  Tuesday noon, or\n",
      "--------------------\n",
      "ay you have prose, ca,\n",
      "Draishengainsterchant,\n",
      "\n",
      "How,--\n",
      "\n",
      "Make, and trumodd\n",
      "STRA\n",
      "PUNThat ever day theredd Vold at,\n",
      "\n",
      "COMIto,\n",
      "\n",
      "\n",
      "ELONYTONIUS\n",
      "\n",
      "\n",
      "And: and the agulies\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      ">>>> ch seem'd too much \n",
      "--------------------\n",
      "ver, and my amenty thening.\n",
      "GUDARUS.\n",
      "\n",
      "\n",
      "Whips in a gock, my modied she donelfore\n",
      "HEntikey, say!\n",
      "Thereethers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ILORDYet\n",
      "Ifore a feengain!\n",
      "ATONYous, and the man\n"
     ]
    }
   ],
   "source": [
    "# get a batch from validation set and generate text\n",
    "context_len = 10\n",
    "batch_gen = 5\n",
    "max_new_tokens = 100\n",
    "\n",
    "context_ids_batch, _ = next(iter(val_loader))\n",
    "context_ids_batch = context_ids_batch.to(cfg_train.device, non_blocking=True)\n",
    "# generate text from the model\n",
    "generated_text = model.generate(context_ids_batch[:batch_gen, :context_len], max_new_tokens=max_new_tokens, temperature=0.9, top_k=20)\n",
    "print(f\">>>> <context>\\n\", \"-\" *20, \"\\n<<<< <generated_text>\")\n",
    "for i, text in enumerate(generated_text):\n",
    "    print(\"\\n\", \"-\" * 80)\n",
    "    print(\">>>>\", decode(text[:context_len].tolist()))\n",
    "    print(\"-\" * 20)\n",
    "    print(\"<<<< \", decode(text[context_len:].tolist()))\n",
    "    if i + 1 >= batch_gen:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
